### Smooth Shading
- We can compute a surface normal at any point on an implicit sphere easily
- But we often deal with polygonal meshes, so if we just use the normal we don't get smooth shading, just flat shading
	- ![[Pasted image 20241023175047.png]]
### Gouraud Shading
- What if we define a different surface normal for each vertex of the triangle, compute their colors, and then interpolate?
	- ![[Pasted image 20241023175251.png]]
- Assume these vertex normals are given
- You can compute them by finding the face normals, and then for each triangle using the vertex, take the average of their normals to find the vertex normal
	- But this isn't the concern of shading, normals should be given
	- This is why stages of the GPU pipeline stages are called shaders, lots of stuff in the GPU was computed this way, it was the OpenGL standard
	- $C = \alpha C_0 + \beta C_1 + \gamma C_2$
- This isn't great though, we can still see triangular edges
- Shading done in the vertex shader
### Phong Shading
- We still have vertex normals, we just interpolate the normals for each point on the triangle, instead of interpolating the colors
	- $\textbf{n}=\frac{\alpha\textbf{n}_0+\beta\textbf{n}_1+\gamma\textbf{n}_2}{|\alpha\textbf{n}_0+\beta\textbf{n}_1+\gamma\textbf{n}_2|}$ (must normalize since the result is not likely normal)
	- ![[Pasted image 20241023175949.png]]
	- ![[Pasted image 20241023182538.png]]
	- Can't tell (except for on the edge) that this surface isn't smooth!
	- Shading done in the fragment shader
### Shading Transformations
- Do shading in the world space, or the view space, because at this point we have the positions of things relative to each other (usually the view space, because sometimes it is better)
- Doesn't matter what space, just as long as all of our vectors are in the same space
- Light source could be defined w.r.t. the camera, so then it is already in the view space!
	- Assume we are using the view space from now on
- ![[Pasted image 20241023185045.png]]
- Now, we are not going straight to canonical view volume, so we need the Model-View transformation $\textbf{M}$ for the shaders
- When transforming our surface normals, they might not be surface normals anymore
- $\textbf{p}'=\textbf{M}\textbf{p}$ for points
- When vertex normals are defined in the model space and we transform them to the view space, if the transformation involves any non-uniform scale, then our vertex normals are no longer necessarily valid (not perpendicular to the surface) and not surface normals anymore!
	- They seemingly rotate the opposite way
- We must apply inverse of the scale transformation to the vertex surface normals
- $\textbf{n}'=?\begin{bmatrix}n_x\\n_y\\n_z\\0\end{bmatrix}$
- If no non-uniform scale, then $?=M_{3x3}$, and just use 3 components of \textbf{n}$
- Remember, $M_{3x3}=R_2 S R_1$, we can decompose a 3x3 matrix this way
- So $M_{normal}=R_2S^{-1}R_1$
	- If we invert $M_{3x3}=R_2SR_1$, we get $M_{3x3}^{-1}=R_1^{-1}S^{-1}R_2^{-1}$
	- Take transpose, $(M_{3x3}^{-1})^T=(R_1^{-1}S^{-1}R_2^{-1})^T$
		- But inverse of a rotation matrix is the transpose, so
	- $(M_{3x3}^{-1})^T=(R_1^TS^{-1}R_2^T)^T=R_2(S^{-1})^TR_1$
		- But scale matrices are diagonal!
	- $(M_{3x3}^{-1})^T=R_2S^{-1}R_1$
	- So $M_{normal}=(M^{-1}_{3x3})^T$
- So $\textbf{n}'=(M^{-1}_{3x3})^T\textbf{n}$ for normal vectors, $\textbf{n}=\begin{bmatrix}n_x\\n_y\\n_z\end{bmatrix}$
- Just only use first 3 columns and first 3 rows of $M$ for the computation, and put in the translation bits in the last column after
- In camera space, view direction is normalized inverse of the transformed position $\textbf{p}'$
	- Where $\textbf{p}'=\textbf{M}\textbf{p}$